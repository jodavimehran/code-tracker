{
  "origin": "codeshovel",
  "repositoryName": "lucene-solr",
  "repositoryPath": "H:\\Projects\\apache\\lucene-solr/.git",
  "startCommitName": "38bf976cd4b9e324c21664bd7ae3d554df803705",
  "sourceFileName": "Field.java",
  "functionName": "tokenStream",
  "functionId": "tokenStream___analyzer-Analyzer__reuse-TokenStream",
  "sourceFilePath": "lucene/core/src/java/org/apache/lucene/document/Field.java",
  "functionStartLine": 472,
  "functionEndLine": 509,
  "numCommitsSeen": 117,
  "timeTaken": 11991,
  "changeHistory": [
    "105c7eae87896762cbcb295c73c8e8b1fd8f71f8",
    "75dd5e9f9e13c72890f1e5b1695f8281fe990d94",
    "7da175b0b6b4185ee6b5df852e59b93d9a9a1c86",
    "249d0d25fec0c8d3aeaa8991b22c96317b6db86a",
    "6bf44e94399e474ba3285d442ce6406cdadc1d9e",
    "bc41d58cd37ab38c1a088ea67197bd3c338ac53f",
    "8f9f8a3252c73428e67bc5d390e58d1370e060ba",
    "1613f1882c00f28f12570e4f75f913a663e1e2c0",
    "43974d668667ba1b1dacf26a18a22c7fea909539",
    "18117c0b04620e0e4bb7403fca5d05d35665de08",
    "f092795fe94ba727f7368b63d8eb1ecd39749fc4",
    "8f88aa64978a61125adafff544c8e5084d497fb5",
    "518fc20d1cf385650202ff9a3b35136ed9d646e5",
    "e53aee7739be1c04bd1673a55b4956efb63c337f",
    "fd16190940d7495e985f44ce7504562c8bbc91e6",
    "854c9ac45223b64acf3e7e4c0a77383a9441268f",
    "eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c",
    "9de01b56ebf252ffefe05e606e330a1787b94c9d",
    "67c13bd2fe57d73a824f163f9c73018fa51a1a65",
    "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
    "ffb3cbee57e1b075dac827bcc46bc95483c603e0",
    "4dad0ba89f1d663939999be9005433dd629955f1",
    "400639f54e54ba2cf90b2436652450ded25861f7",
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
    "efb74380fda0da18650dbc66372a7bb1cd41dcf6",
    "edfce675a575bf83baecb52ca58c6e307d50eaad"
  ],
  "changeHistoryShort": {
    "105c7eae87896762cbcb295c73c8e8b1fd8f71f8": "Ybodychange",
    "75dd5e9f9e13c72890f1e5b1695f8281fe990d94": "Yexceptionschange",
    "7da175b0b6b4185ee6b5df852e59b93d9a9a1c86": "Ybodychange",
    "249d0d25fec0c8d3aeaa8991b22c96317b6db86a": "Ybodychange",
    "6bf44e94399e474ba3285d442ce6406cdadc1d9e": "Ybodychange",
    "bc41d58cd37ab38c1a088ea67197bd3c338ac53f": "Ybodychange",
    "8f9f8a3252c73428e67bc5d390e58d1370e060ba": "Ybodychange",
    "1613f1882c00f28f12570e4f75f913a663e1e2c0": "Ymultichange(Yparameterchange,Ybodychange)",
    "43974d668667ba1b1dacf26a18a22c7fea909539": "Ybodychange",
    "18117c0b04620e0e4bb7403fca5d05d35665de08": "Ybodychange",
    "f092795fe94ba727f7368b63d8eb1ecd39749fc4": "Ybodychange",
    "8f88aa64978a61125adafff544c8e5084d497fb5": "Ybodychange",
    "518fc20d1cf385650202ff9a3b35136ed9d646e5": "Ybodychange",
    "e53aee7739be1c04bd1673a55b4956efb63c337f": "Ybodychange",
    "fd16190940d7495e985f44ce7504562c8bbc91e6": "Ybodychange",
    "854c9ac45223b64acf3e7e4c0a77383a9441268f": "Ybodychange",
    "eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c": "Yfilerename",
    "9de01b56ebf252ffefe05e606e330a1787b94c9d": "Ybodychange",
    "67c13bd2fe57d73a824f163f9c73018fa51a1a65": "Ybodychange",
    "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027": "Ymultichange(Ymovefromfile,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
    "ffb3cbee57e1b075dac827bcc46bc95483c603e0": "Ybodychange",
    "4dad0ba89f1d663939999be9005433dd629955f1": "Ybodychange",
    "400639f54e54ba2cf90b2436652450ded25861f7": "Ybodychange",
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec": "Yfilerename",
    "efb74380fda0da18650dbc66372a7bb1cd41dcf6": "Ybodychange",
    "edfce675a575bf83baecb52ca58c6e307d50eaad": "Yintroduced"
  },
  "changeHistoryDetails": {
    "105c7eae87896762cbcb295c73c8e8b1fd8f71f8": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-7413: move legacy numeric support to backwards module\n",
      "commitDate": "2016-08-17, 9:28 a.m.",
      "commitName": "105c7eae87896762cbcb295c73c8e8b1fd8f71f8",
      "commitAuthor": "Robert Muir",
      "commitDateOld": "2016-08-02, 5:10 a.m.",
      "commitNameOld": "d9df295bb73e011b72425d62ce609a14e4644aa4",
      "commitAuthorOld": "Mike McCandless",
      "daysBetweenCommits": 15.18,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n      // Not indexed\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() !\u003d null) {\n        if (!(reuse instanceof StringTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new StringTokenStream();\n        }\n        ((StringTokenStream) reuse).setValue(stringValue());\n        return reuse;\n      } else if (binaryValue() !\u003d null) {\n        if (!(reuse instanceof BinaryTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new BinaryTokenStream();\n        }\n        ((BinaryTokenStream) reuse).setValue(binaryValue());\n        return reuse;\n      } else {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 504,
      "functionName": "tokenStream",
      "diff": "@@ -1,67 +1,38 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n     if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n-    final FieldType.LegacyNumericType numericType \u003d fieldType().numericType();\n-    if (numericType !\u003d null) {\n-      if (!(reuse instanceof LegacyNumericTokenStream \u0026\u0026 ((LegacyNumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n-        // lazy init the TokenStream as it is heavy to instantiate\n-        // (attributes,...) if not needed (stored field loading)\n-        reuse \u003d new LegacyNumericTokenStream(type.numericPrecisionStep());\n-      }\n-      final LegacyNumericTokenStream nts \u003d (LegacyNumericTokenStream) reuse;\n-      // initialize value in TokenStream\n-      final Number val \u003d (Number) fieldsData;\n-      switch (numericType) {\n-      case INT:\n-        nts.setIntValue(val.intValue());\n-        break;\n-      case LONG:\n-        nts.setLongValue(val.longValue());\n-        break;\n-      case FLOAT:\n-        nts.setFloatValue(val.floatValue());\n-        break;\n-      case DOUBLE:\n-        nts.setDoubleValue(val.doubleValue());\n-        break;\n-      default:\n-        throw new AssertionError(\"Should never get here\");\n-      }\n-      return reuse;\n-    }\n-\n     if (!fieldType().tokenized()) {\n       if (stringValue() !\u003d null) {\n         if (!(reuse instanceof StringTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse \u003d new StringTokenStream();\n         }\n         ((StringTokenStream) reuse).setValue(stringValue());\n         return reuse;\n       } else if (binaryValue() !\u003d null) {\n         if (!(reuse instanceof BinaryTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse \u003d new BinaryTokenStream();\n         }\n         ((BinaryTokenStream) reuse).setValue(binaryValue());\n         return reuse;\n       } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "75dd5e9f9e13c72890f1e5b1695f8281fe990d94": {
      "type": "Yexceptionschange",
      "commitMessage": "LUCENE-6988: IndexableField.tokenStream() no longer throws IOException\n",
      "commitDate": "2016-01-25, 5:04 a.m.",
      "commitName": "75dd5e9f9e13c72890f1e5b1695f8281fe990d94",
      "commitAuthor": "Alan Woodward",
      "commitDateOld": "2016-01-17, 2:54 p.m.",
      "commitNameOld": "24c46305bd8f335c3d0e501a33dd3da82732c49e",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 7.59,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n      // Not indexed\n      return null;\n    }\n\n    final FieldType.LegacyNumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof LegacyNumericTokenStream \u0026\u0026 ((LegacyNumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new LegacyNumericTokenStream(type.numericPrecisionStep());\n      }\n      final LegacyNumericTokenStream nts \u003d (LegacyNumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() !\u003d null) {\n        if (!(reuse instanceof StringTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new StringTokenStream();\n        }\n        ((StringTokenStream) reuse).setValue(stringValue());\n        return reuse;\n      } else if (binaryValue() !\u003d null) {\n        if (!(reuse instanceof BinaryTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new BinaryTokenStream();\n        }\n        ((BinaryTokenStream) reuse).setValue(binaryValue());\n        return reuse;\n      } else {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 499,
      "functionName": "tokenStream",
      "diff": "@@ -1,67 +1,67 @@\n-  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n+  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) {\n     if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n     final FieldType.LegacyNumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(reuse instanceof LegacyNumericTokenStream \u0026\u0026 ((LegacyNumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new LegacyNumericTokenStream(type.numericPrecisionStep());\n       }\n       final LegacyNumericTokenStream nts \u003d (LegacyNumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() !\u003d null) {\n         if (!(reuse instanceof StringTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse \u003d new StringTokenStream();\n         }\n         ((StringTokenStream) reuse).setValue(stringValue());\n         return reuse;\n       } else if (binaryValue() !\u003d null) {\n         if (!(reuse instanceof BinaryTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse \u003d new BinaryTokenStream();\n         }\n         ((BinaryTokenStream) reuse).setValue(binaryValue());\n         return reuse;\n       } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {
        "oldValue": "[IOException]",
        "newValue": "[]"
      }
    },
    "7da175b0b6b4185ee6b5df852e59b93d9a9a1c86": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-6917: rename/deprecate numeric classes in favor of dimensional values\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1719562 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2015-12-11, 4:13 p.m.",
      "commitName": "7da175b0b6b4185ee6b5df852e59b93d9a9a1c86",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2015-07-02, 10:09 a.m.",
      "commitNameOld": "1816ed194556a8199c0338272d1c894036aa163f",
      "commitAuthorOld": "Uwe Schindler",
      "daysBetweenCommits": 162.29,
      "commitsBetweenForRepo": 875,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n      // Not indexed\n      return null;\n    }\n\n    final FieldType.LegacyNumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof LegacyNumericTokenStream \u0026\u0026 ((LegacyNumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new LegacyNumericTokenStream(type.numericPrecisionStep());\n      }\n      final LegacyNumericTokenStream nts \u003d (LegacyNumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() !\u003d null) {\n        if (!(reuse instanceof StringTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new StringTokenStream();\n        }\n        ((StringTokenStream) reuse).setValue(stringValue());\n        return reuse;\n      } else if (binaryValue() !\u003d null) {\n        if (!(reuse instanceof BinaryTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new BinaryTokenStream();\n        }\n        ((BinaryTokenStream) reuse).setValue(binaryValue());\n        return reuse;\n      } else {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 500,
      "functionName": "tokenStream",
      "diff": "@@ -1,67 +1,67 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n-    final NumericType numericType \u003d fieldType().numericType();\n+    final FieldType.LegacyNumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n-      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n+      if (!(reuse instanceof LegacyNumericTokenStream \u0026\u0026 ((LegacyNumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n+        reuse \u003d new LegacyNumericTokenStream(type.numericPrecisionStep());\n       }\n-      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n+      final LegacyNumericTokenStream nts \u003d (LegacyNumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() !\u003d null) {\n         if (!(reuse instanceof StringTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse \u003d new StringTokenStream();\n         }\n         ((StringTokenStream) reuse).setValue(stringValue());\n         return reuse;\n       } else if (binaryValue() !\u003d null) {\n         if (!(reuse instanceof BinaryTokenStream)) {\n           // lazy init the TokenStream as it is heavy to instantiate\n           // (attributes,...) if not needed\n           reuse \u003d new BinaryTokenStream();\n         }\n         ((BinaryTokenStream) reuse).setValue(binaryValue());\n         return reuse;\n       } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "249d0d25fec0c8d3aeaa8991b22c96317b6db86a": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-5989: allow passing BytesRef to StringField to make it easier to index arbitrary binary tokens\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1672781 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2015-04-10, 6:24 p.m.",
      "commitName": "249d0d25fec0c8d3aeaa8991b22c96317b6db86a",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2015-02-07, 5:10 a.m.",
      "commitNameOld": "376256316b016f4971bfa86517c750fae42158c7",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 62.51,
      "commitsBetweenForRepo": 511,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n      // Not indexed\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() !\u003d null) {\n        if (!(reuse instanceof StringTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new StringTokenStream();\n        }\n        ((StringTokenStream) reuse).setValue(stringValue());\n        return reuse;\n      } else if (binaryValue() !\u003d null) {\n        if (!(reuse instanceof BinaryTokenStream)) {\n          // lazy init the TokenStream as it is heavy to instantiate\n          // (attributes,...) if not needed\n          reuse \u003d new BinaryTokenStream();\n        }\n        ((BinaryTokenStream) reuse).setValue(binaryValue());\n        return reuse;\n      } else {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 504,
      "functionName": "tokenStream",
      "diff": "@@ -1,58 +1,67 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n-      if (stringValue() \u003d\u003d null) {\n+      if (stringValue() !\u003d null) {\n+        if (!(reuse instanceof StringTokenStream)) {\n+          // lazy init the TokenStream as it is heavy to instantiate\n+          // (attributes,...) if not needed\n+          reuse \u003d new StringTokenStream();\n+        }\n+        ((StringTokenStream) reuse).setValue(stringValue());\n+        return reuse;\n+      } else if (binaryValue() !\u003d null) {\n+        if (!(reuse instanceof BinaryTokenStream)) {\n+          // lazy init the TokenStream as it is heavy to instantiate\n+          // (attributes,...) if not needed\n+          reuse \u003d new BinaryTokenStream();\n+        }\n+        ((BinaryTokenStream) reuse).setValue(binaryValue());\n+        return reuse;\n+      } else {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-      if (!(reuse instanceof StringTokenStream)) {\n-        // lazy init the TokenStream as it is heavy to instantiate\n-        // (attributes,...) if not needed (stored field loading)\n-        reuse \u003d new StringTokenStream();\n-      }\n-      ((StringTokenStream) reuse).setValue(stringValue());\n-      return reuse;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "6bf44e94399e474ba3285d442ce6406cdadc1d9e": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-6039: NO -\u003e NONE\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1635861 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2014-10-31, 4:48 p.m.",
      "commitName": "6bf44e94399e474ba3285d442ce6406cdadc1d9e",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2014-10-31, 11:10 a.m.",
      "commitNameOld": "bc41d58cd37ab38c1a088ea67197bd3c338ac53f",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 0.23,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n      // Not indexed\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(reuse instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) reuse).setValue(stringValue());\n      return reuse;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 505,
      "functionName": "tokenStream",
      "diff": "@@ -1,58 +1,58 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n-    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NO) {\n+    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NONE) {\n       // Not indexed\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) reuse).setValue(stringValue());\n       return reuse;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "bc41d58cd37ab38c1a088ea67197bd3c338ac53f": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-6039: cutover to IndexOptions.NO/DocValuesType.NO instead of null\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1635790 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2014-10-31, 11:10 a.m.",
      "commitName": "bc41d58cd37ab38c1a088ea67197bd3c338ac53f",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2014-10-21, 3:32 a.m.",
      "commitNameOld": "8f9f8a3252c73428e67bc5d390e58d1370e060ba",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 10.32,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NO) {\n      // Not indexed\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(reuse instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) reuse).setValue(stringValue());\n      return reuse;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 505,
      "functionName": "tokenStream",
      "diff": "@@ -1,58 +1,58 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n-    if (fieldType().indexOptions() \u003d\u003d null) {\n+    if (fieldType().indexOptions() \u003d\u003d IndexOptions.NO) {\n       // Not indexed\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) reuse).setValue(stringValue());\n       return reuse;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "8f9f8a3252c73428e67bc5d390e58d1370e060ba": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-6013: remove IndexableFieldType.indexed and FieldInfo.indexed (it\u0027s redundant with IndexOptions !\u003d null)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1633296 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2014-10-21, 3:32 a.m.",
      "commitName": "8f9f8a3252c73428e67bc5d390e58d1370e060ba",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2014-05-09, 12:36 p.m.",
      "commitNameOld": "e12039a377c4639f30aad8b31fb39964754d6084",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 164.62,
      "commitsBetweenForRepo": 1136,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n    if (fieldType().indexOptions() \u003d\u003d null) {\n      // Not indexed\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(reuse instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) reuse).setValue(stringValue());\n      return reuse;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 504,
      "functionName": "tokenStream",
      "diff": "@@ -1,57 +1,58 @@\n   public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n-    if (!fieldType().indexed()) {\n+    if (fieldType().indexOptions() \u003d\u003d null) {\n+      // Not indexed\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         reuse \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) reuse).setValue(stringValue());\n       return reuse;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "1613f1882c00f28f12570e4f75f913a663e1e2c0": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "LUCENE-5634: Reuse TokenStream instances for string and numeric Fields\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1591992 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2014-05-02, 2:07 p.m.",
      "commitName": "1613f1882c00f28f12570e4f75f913a663e1e2c0",
      "commitAuthor": "Robert Muir",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "LUCENE-5634: Reuse TokenStream instances for string and numeric Fields\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1591992 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2014-05-02, 2:07 p.m.",
          "commitName": "1613f1882c00f28f12570e4f75f913a663e1e2c0",
          "commitAuthor": "Robert Muir",
          "commitDateOld": "2014-04-29, 1:18 p.m.",
          "commitNameOld": "f5de5d01e8108653b2d6e2dcb0f7e9af7389937a",
          "commitAuthorOld": "",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 36,
          "commitsBetweenForFile": 1,
          "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(reuse instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) reuse).setValue(stringValue());\n      return reuse;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
          "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
          "functionStartLine": 495,
          "functionName": "tokenStream",
          "diff": "@@ -1,57 +1,57 @@\n-  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n-      if (!(internalTokenStream instanceof NumericTokenStream)) {\n+      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n+        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n-      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n+      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n-      return internalTokenStream;\n+      return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-      if (!(internalTokenStream instanceof StringTokenStream)) {\n+      if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream \u003d new StringTokenStream();\n+        reuse \u003d new StringTokenStream();\n       }\n-      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n-      return internalTokenStream;\n+      ((StringTokenStream) reuse).setValue(stringValue());\n+      return reuse;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "[analyzer-Analyzer]",
            "newValue": "[analyzer-Analyzer, reuse-TokenStream]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "LUCENE-5634: Reuse TokenStream instances for string and numeric Fields\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1591992 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2014-05-02, 2:07 p.m.",
          "commitName": "1613f1882c00f28f12570e4f75f913a663e1e2c0",
          "commitAuthor": "Robert Muir",
          "commitDateOld": "2014-04-29, 1:18 p.m.",
          "commitNameOld": "f5de5d01e8108653b2d6e2dcb0f7e9af7389937a",
          "commitAuthorOld": "",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 36,
          "commitsBetweenForFile": 1,
          "actualSource": "  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return reuse;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(reuse instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        reuse \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) reuse).setValue(stringValue());\n      return reuse;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
          "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
          "functionStartLine": 495,
          "functionName": "tokenStream",
          "diff": "@@ -1,57 +1,57 @@\n-  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n-      if (!(internalTokenStream instanceof NumericTokenStream)) {\n+      if (!(reuse instanceof NumericTokenStream \u0026\u0026 ((NumericTokenStream)reuse).getPrecisionStep() \u003d\u003d type.numericPrecisionStep())) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n+        reuse \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n-      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n+      final NumericTokenStream nts \u003d (NumericTokenStream) reuse;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n-      return internalTokenStream;\n+      return reuse;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-      if (!(internalTokenStream instanceof StringTokenStream)) {\n+      if (!(reuse instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        internalTokenStream \u003d new StringTokenStream();\n+        reuse \u003d new StringTokenStream();\n       }\n-      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n-      return internalTokenStream;\n+      ((StringTokenStream) reuse).setValue(stringValue());\n+      return reuse;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {}
        }
      ]
    },
    "43974d668667ba1b1dacf26a18a22c7fea909539": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-5339: javadocs\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5339@1554710 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2014-01-01, 7:23 p.m.",
      "commitName": "43974d668667ba1b1dacf26a18a22c7fea909539",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2013-11-18, 9:53 a.m.",
      "commitNameOld": "18117c0b04620e0e4bb7403fca5d05d35665de08",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 44.4,
      "commitsBetweenForRepo": 225,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(internalTokenStream instanceof NumericTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return internalTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(internalTokenStream instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n      return internalTokenStream;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 502,
      "functionName": "tokenStream",
      "diff": "@@ -1,57 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n-    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; this\u003d\" + this);\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; got \" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "18117c0b04620e0e4bb7403fca5d05d35665de08": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-5339: assocations\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5339@1543047 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2013-11-18, 9:53 a.m.",
      "commitName": "18117c0b04620e0e4bb7403fca5d05d35665de08",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2013-07-08, 1:55 p.m.",
      "commitNameOld": "f092795fe94ba727f7368b63d8eb1ecd39749fc4",
      "commitAuthorOld": "Uwe Schindler",
      "daysBetweenCommits": 132.87,
      "commitsBetweenForRepo": 835,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(internalTokenStream instanceof NumericTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return internalTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(internalTokenStream instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n      return internalTokenStream;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; this\u003d\" + this);\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 502,
      "functionName": "tokenStream",
      "diff": "@@ -1,57 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), stringValue());\n     }\n \n-    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value; this\u003d\" + this);\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "f092795fe94ba727f7368b63d8eb1ecd39749fc4": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-5097: Analyzer now has an additional tokenStream(String fieldName, String text) method, so wrapping by StringReader for common use is no longer needed. This method uses an internal reuseable reader, which was previously only used by the Field class.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500862 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2013-07-08, 1:55 p.m.",
      "commitName": "f092795fe94ba727f7368b63d8eb1ecd39749fc4",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2013-01-22, 1:36 a.m.",
      "commitNameOld": "06bf9a0857e997e7c5251ca8546556d1cdadf80f",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 167.47,
      "commitsBetweenForRepo": 1345,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(internalTokenStream instanceof NumericTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return internalTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(internalTokenStream instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n      return internalTokenStream;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), stringValue());\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 502,
      "functionName": "tokenStream",
      "diff": "@@ -1,61 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n-      if (internalReader \u003d\u003d null) {\n-        internalReader \u003d new ReusableStringReader();\n-      }\n-      internalReader.setValue(stringValue());\n-      return analyzer.tokenStream(name(), internalReader);\n+      return analyzer.tokenStream(name(), stringValue());\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "8f88aa64978a61125adafff544c8e5084d497fb5": {
      "type": "Ybodychange",
      "commitMessage": "Turn some \"assert false\" in switch and switch-like statements into AssertionErrors. If we get into the default block we are wrong and we can also throw Ex, because then there is logic error (e.g. after new enum constant was added)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1379450 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-08-31, 9:55 a.m.",
      "commitName": "8f88aa64978a61125adafff544c8e5084d497fb5",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2012-08-30, 10:33 a.m.",
      "commitNameOld": "a4702d3711e8ad2ac5ef455e4182f2da80011c86",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 0.97,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(internalTokenStream instanceof NumericTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        throw new AssertionError(\"Should never get here\");\n      }\n      return internalTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(internalTokenStream instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n      return internalTokenStream;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      if (internalReader \u003d\u003d null) {\n        internalReader \u003d new ReusableStringReader();\n      }\n      internalReader.setValue(stringValue());\n      return analyzer.tokenStream(name(), internalReader);\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 495,
      "functionName": "tokenStream",
      "diff": "@@ -1,61 +1,61 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n-        assert false : \"Should never get here\";\n+        throw new AssertionError(\"Should never get here\");\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       if (internalReader \u003d\u003d null) {\n         internalReader \u003d new ReusableStringReader();\n       }\n       internalReader.setValue(stringValue());\n       return analyzer.tokenStream(name(), internalReader);\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "518fc20d1cf385650202ff9a3b35136ed9d646e5": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-4315: Add ReusableStringReader to Field.java\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1376261 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-08-22, 5:29 p.m.",
      "commitName": "518fc20d1cf385650202ff9a3b35136ed9d646e5",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2012-08-21, 8:43 a.m.",
      "commitNameOld": "e53aee7739be1c04bd1673a55b4956efb63c337f",
      "commitAuthorOld": "Uwe Schindler",
      "daysBetweenCommits": 1.37,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(internalTokenStream instanceof NumericTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        assert false : \"Should never get here\";\n      }\n      return internalTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(internalTokenStream instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n      return internalTokenStream;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      if (internalReader \u003d\u003d null) {\n        internalReader \u003d new ReusableStringReader();\n      }\n      internalReader.setValue(stringValue());\n      return analyzer.tokenStream(name(), internalReader);\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 411,
      "functionName": "tokenStream",
      "diff": "@@ -1,57 +1,61 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n       final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n       // initialize value in TokenStream\n       final Number val \u003d (Number) fieldsData;\n       switch (numericType) {\n       case INT:\n         nts.setIntValue(val.intValue());\n         break;\n       case LONG:\n         nts.setLongValue(val.longValue());\n         break;\n       case FLOAT:\n         nts.setFloatValue(val.floatValue());\n         break;\n       case DOUBLE:\n         nts.setDoubleValue(val.doubleValue());\n         break;\n       default:\n         assert false : \"Should never get here\";\n       }\n       return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n       if (!(internalTokenStream instanceof StringTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         internalTokenStream \u003d new StringTokenStream();\n       }\n       ((StringTokenStream) internalTokenStream).setValue(stringValue());\n       return internalTokenStream;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n-      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n+      if (internalReader \u003d\u003d null) {\n+        internalReader \u003d new ReusableStringReader();\n+      }\n+      internalReader.setValue(stringValue());\n+      return analyzer.tokenStream(name(), internalReader);\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "e53aee7739be1c04bd1673a55b4956efb63c337f": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-4317: Improve reuse of internal TokenStreams in oal.document.Field.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1375507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-08-21, 8:43 a.m.",
      "commitName": "e53aee7739be1c04bd1673a55b4956efb63c337f",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2012-08-03, 4:26 p.m.",
      "commitNameOld": "8f726e254bc060d45590f72411db69c943e1216b",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 17.68,
      "commitsBetweenForRepo": 211,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (!(internalTokenStream instanceof NumericTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n      }\n      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n      // initialize value in TokenStream\n      final Number val \u003d (Number) fieldsData;\n      switch (numericType) {\n      case INT:\n        nts.setIntValue(val.intValue());\n        break;\n      case LONG:\n        nts.setLongValue(val.longValue());\n        break;\n      case FLOAT:\n        nts.setFloatValue(val.floatValue());\n        break;\n      case DOUBLE:\n        nts.setDoubleValue(val.doubleValue());\n        break;\n      default:\n        assert false : \"Should never get here\";\n      }\n      return internalTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n      if (!(internalTokenStream instanceof StringTokenStream)) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        internalTokenStream \u003d new StringTokenStream();\n      }\n      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n      return internalTokenStream;\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 411,
      "functionName": "tokenStream",
      "diff": "@@ -1,75 +1,57 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n-      if (numericTokenStream \u003d\u003d null) {\n+      if (!(internalTokenStream instanceof NumericTokenStream)) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n-        numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n-        // initialize value in TokenStream\n-        final Number val \u003d (Number) fieldsData;\n-        switch (numericType) {\n-        case INT:\n-          numericTokenStream.setIntValue(val.intValue());\n-          break;\n-        case LONG:\n-          numericTokenStream.setLongValue(val.longValue());\n-          break;\n-        case FLOAT:\n-          numericTokenStream.setFloatValue(val.floatValue());\n-          break;\n-        case DOUBLE:\n-          numericTokenStream.setDoubleValue(val.doubleValue());\n-          break;\n-        default:\n-          assert false : \"Should never get here\";\n-        }\n-      } else {\n-        // OK -- previously cached and we already updated if\n-        // setters were called.\n+        internalTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n       }\n-\n-      return numericTokenStream;\n+      final NumericTokenStream nts \u003d (NumericTokenStream) internalTokenStream;\n+      // initialize value in TokenStream\n+      final Number val \u003d (Number) fieldsData;\n+      switch (numericType) {\n+      case INT:\n+        nts.setIntValue(val.intValue());\n+        break;\n+      case LONG:\n+        nts.setLongValue(val.longValue());\n+        break;\n+      case FLOAT:\n+        nts.setFloatValue(val.floatValue());\n+        break;\n+      case DOUBLE:\n+        nts.setDoubleValue(val.doubleValue());\n+        break;\n+      default:\n+        assert false : \"Should never get here\";\n+      }\n+      return internalTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n-\n-      return new TokenStream() {\n-        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n-        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n-        boolean used;\n-\n-        @Override\n-        public boolean incrementToken() {\n-          if (used) {\n-            return false;\n-          }\n-          termAttribute.setEmpty().append(stringValue());\n-          offsetAttribute.setOffset(0, stringValue().length());\n-          used \u003d true;\n-          return true;\n-        }\n-\n-        @Override\n-        public void reset() {\n-          used \u003d false;\n-        }\n-      };\n+      if (!(internalTokenStream instanceof StringTokenStream)) {\n+        // lazy init the TokenStream as it is heavy to instantiate\n+        // (attributes,...) if not needed (stored field loading)\n+        internalTokenStream \u003d new StringTokenStream();\n+      }\n+      ((StringTokenStream) internalTokenStream).setValue(stringValue());\n+      return internalTokenStream;\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "fd16190940d7495e985f44ce7504562c8bbc91e6": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-4172: clean up redundant throws clauses\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1355069 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-06-28, 12:39 p.m.",
      "commitName": "fd16190940d7495e985f44ce7504562c8bbc91e6",
      "commitAuthor": "Steven Rowe",
      "commitDateOld": "2012-06-11, 3:26 p.m.",
      "commitNameOld": "2ac3eb27c41691c7a61b58673be6fe9a5a0eed9e",
      "commitAuthorOld": "Chris M. Hostetter",
      "daysBetweenCommits": 16.88,
      "commitsBetweenForRepo": 110,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (numericTokenStream \u003d\u003d null) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n        // initialize value in TokenStream\n        final Number val \u003d (Number) fieldsData;\n        switch (numericType) {\n        case INT:\n          numericTokenStream.setIntValue(val.intValue());\n          break;\n        case LONG:\n          numericTokenStream.setLongValue(val.longValue());\n          break;\n        case FLOAT:\n          numericTokenStream.setFloatValue(val.floatValue());\n          break;\n        case DOUBLE:\n          numericTokenStream.setDoubleValue(val.doubleValue());\n          break;\n        default:\n          assert false : \"Should never get here\";\n        }\n      } else {\n        // OK -- previously cached and we already updated if\n        // setters were called.\n      }\n\n      return numericTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 424,
      "functionName": "tokenStream",
      "diff": "@@ -1,75 +1,75 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (numericTokenStream \u003d\u003d null) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n         // initialize value in TokenStream\n         final Number val \u003d (Number) fieldsData;\n         switch (numericType) {\n         case INT:\n           numericTokenStream.setIntValue(val.intValue());\n           break;\n         case LONG:\n           numericTokenStream.setLongValue(val.longValue());\n           break;\n         case FLOAT:\n           numericTokenStream.setFloatValue(val.floatValue());\n           break;\n         case DOUBLE:\n           numericTokenStream.setDoubleValue(val.doubleValue());\n           break;\n         default:\n           assert false : \"Should never get here\";\n         }\n       } else {\n         // OK -- previously cached and we already updated if\n         // setters were called.\n       }\n \n       return numericTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n-        public boolean incrementToken() throws IOException {\n+        public boolean incrementToken() {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used \u003d true;\n           return true;\n         }\n \n         @Override\n-        public void reset() throws IOException {\n+        public void reset() {\n           used \u003d false;\n         }\n       };\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "854c9ac45223b64acf3e7e4c0a77383a9441268f": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-3777: separate out Int/Long/Float/DoubleField to reduce traps\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1245583 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-02-17, 9:46 a.m.",
      "commitName": "854c9ac45223b64acf3e7e4c0a77383a9441268f",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2012-02-07, 2:59 p.m.",
      "commitNameOld": "eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c",
      "commitAuthorOld": "Steven Rowe",
      "daysBetweenCommits": 9.78,
      "commitsBetweenForRepo": 124,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (numericTokenStream \u003d\u003d null) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n        // initialize value in TokenStream\n        final Number val \u003d (Number) fieldsData;\n        switch (numericType) {\n        case INT:\n          numericTokenStream.setIntValue(val.intValue());\n          break;\n        case LONG:\n          numericTokenStream.setLongValue(val.longValue());\n          break;\n        case FLOAT:\n          numericTokenStream.setFloatValue(val.floatValue());\n          break;\n        case DOUBLE:\n          numericTokenStream.setDoubleValue(val.doubleValue());\n          break;\n        default:\n          assert false : \"Should never get here\";\n        }\n      } else {\n        // OK -- previously cached and we already updated if\n        // setters were called.\n      }\n\n      return numericTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 397,
      "functionName": "tokenStream",
      "diff": "@@ -1,75 +1,75 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n-    final NumericField.DataType numericType \u003d fieldType().numericType();\n+    final NumericType numericType \u003d fieldType().numericType();\n     if (numericType !\u003d null) {\n       if (numericTokenStream \u003d\u003d null) {\n         // lazy init the TokenStream as it is heavy to instantiate\n         // (attributes,...) if not needed (stored field loading)\n         numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n         // initialize value in TokenStream\n         final Number val \u003d (Number) fieldsData;\n         switch (numericType) {\n         case INT:\n           numericTokenStream.setIntValue(val.intValue());\n           break;\n         case LONG:\n           numericTokenStream.setLongValue(val.longValue());\n           break;\n         case FLOAT:\n           numericTokenStream.setFloatValue(val.floatValue());\n           break;\n         case DOUBLE:\n           numericTokenStream.setDoubleValue(val.doubleValue());\n           break;\n         default:\n           assert false : \"Should never get here\";\n         }\n       } else {\n         // OK -- previously cached and we already updated if\n         // setters were called.\n       }\n \n       return numericTokenStream;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n         public boolean incrementToken() throws IOException {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used \u003d true;\n           return true;\n         }\n \n         @Override\n         public void reset() throws IOException {\n           used \u003d false;\n         }\n       };\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c": {
      "type": "Yfilerename",
      "commitMessage": "LUCENE-3753: Restructure the Lucene build system\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1241588 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-02-07, 2:59 p.m.",
      "commitName": "eb0ab3d392a42c1835f79bcd7f5404bcc50c8e4c",
      "commitAuthor": "Steven Rowe",
      "commitDateOld": "2012-02-07, 1:58 p.m.",
      "commitNameOld": "8b939cb7d20160f9f8a7baf2030613f0e1e877b4",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericField.DataType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (numericTokenStream \u003d\u003d null) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n        // initialize value in TokenStream\n        final Number val \u003d (Number) fieldsData;\n        switch (numericType) {\n        case INT:\n          numericTokenStream.setIntValue(val.intValue());\n          break;\n        case LONG:\n          numericTokenStream.setLongValue(val.longValue());\n          break;\n        case FLOAT:\n          numericTokenStream.setFloatValue(val.floatValue());\n          break;\n        case DOUBLE:\n          numericTokenStream.setDoubleValue(val.doubleValue());\n          break;\n        default:\n          assert false : \"Should never get here\";\n        }\n      } else {\n        // OK -- previously cached and we already updated if\n        // setters were called.\n      }\n\n      return numericTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/core/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 441,
      "functionName": "tokenStream",
      "diff": "",
      "extendedDetails": {
        "oldPath": "lucene/src/java/org/apache/lucene/document/Field.java",
        "newPath": "lucene/core/src/java/org/apache/lucene/document/Field.java"
      }
    },
    "9de01b56ebf252ffefe05e606e330a1787b94c9d": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-3453: simplify DocValues/Field API\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1231791 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2012-01-15, 6:05 p.m.",
      "commitName": "9de01b56ebf252ffefe05e606e330a1787b94c9d",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2011-12-10, 1:17 p.m.",
      "commitNameOld": "124728c97487911ab225397878e15c5469ba0360",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 36.2,
      "commitsBetweenForRepo": 163,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    final NumericField.DataType numericType \u003d fieldType().numericType();\n    if (numericType !\u003d null) {\n      if (numericTokenStream \u003d\u003d null) {\n        // lazy init the TokenStream as it is heavy to instantiate\n        // (attributes,...) if not needed (stored field loading)\n        numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n        // initialize value in TokenStream\n        final Number val \u003d (Number) fieldsData;\n        switch (numericType) {\n        case INT:\n          numericTokenStream.setIntValue(val.intValue());\n          break;\n        case LONG:\n          numericTokenStream.setLongValue(val.longValue());\n          break;\n        case FLOAT:\n          numericTokenStream.setFloatValue(val.floatValue());\n          break;\n        case DOUBLE:\n          numericTokenStream.setDoubleValue(val.doubleValue());\n          break;\n        default:\n          assert false : \"Should never get here\";\n        }\n      } else {\n        // OK -- previously cached and we already updated if\n        // setters were called.\n      }\n\n      return numericTokenStream;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n  }",
      "path": "lucene/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 440,
      "functionName": "tokenStream",
      "diff": "@@ -1,43 +1,75 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n+    final NumericField.DataType numericType \u003d fieldType().numericType();\n+    if (numericType !\u003d null) {\n+      if (numericTokenStream \u003d\u003d null) {\n+        // lazy init the TokenStream as it is heavy to instantiate\n+        // (attributes,...) if not needed (stored field loading)\n+        numericTokenStream \u003d new NumericTokenStream(type.numericPrecisionStep());\n+        // initialize value in TokenStream\n+        final Number val \u003d (Number) fieldsData;\n+        switch (numericType) {\n+        case INT:\n+          numericTokenStream.setIntValue(val.intValue());\n+          break;\n+        case LONG:\n+          numericTokenStream.setLongValue(val.longValue());\n+          break;\n+        case FLOAT:\n+          numericTokenStream.setFloatValue(val.floatValue());\n+          break;\n+        case DOUBLE:\n+          numericTokenStream.setDoubleValue(val.doubleValue());\n+          break;\n+        default:\n+          assert false : \"Should never get here\";\n+        }\n+      } else {\n+        // OK -- previously cached and we already updated if\n+        // setters were called.\n+      }\n+\n+      return numericTokenStream;\n+    }\n+\n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n         public boolean incrementToken() throws IOException {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used \u003d true;\n           return true;\n         }\n \n         @Override\n         public void reset() throws IOException {\n           used \u003d false;\n         }\n       };\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n       return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n       return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n-    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String, Reader or Number value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "67c13bd2fe57d73a824f163f9c73018fa51a1a65": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-3455: Renamed Analyzer.reusableTokenStream to Analyzer.tokenStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1176728 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2011-09-28, 1:26 a.m.",
      "commitName": "67c13bd2fe57d73a824f163f9c73018fa51a1a65",
      "commitAuthor": "Christopher John Male",
      "commitDateOld": "2011-09-22, 11:16 p.m.",
      "commitNameOld": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
      "commitAuthorOld": "Christopher John Male",
      "daysBetweenCommits": 5.09,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.tokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }",
      "path": "lucene/src/java/org/apache/lucene/document/Field.java",
      "functionStartLine": 317,
      "functionName": "tokenStream",
      "diff": "@@ -1,43 +1,43 @@\n   public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n     if (!fieldType().indexed()) {\n       return null;\n     }\n \n     if (!fieldType().tokenized()) {\n       if (stringValue() \u003d\u003d null) {\n         throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n       }\n \n       return new TokenStream() {\n         CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n         OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n         boolean used;\n \n         @Override\n         public boolean incrementToken() throws IOException {\n           if (used) {\n             return false;\n           }\n           termAttribute.setEmpty().append(stringValue());\n           offsetAttribute.setOffset(0, stringValue().length());\n           used \u003d true;\n           return true;\n         }\n \n         @Override\n         public void reset() throws IOException {\n           used \u003d false;\n         }\n       };\n     }\n \n     if (tokenStream !\u003d null) {\n       return tokenStream;\n     } else if (readerValue() !\u003d null) {\n-      return analyzer.reusableTokenStream(name(), readerValue());\n+      return analyzer.tokenStream(name(), readerValue());\n     } else if (stringValue() !\u003d null) {\n-      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+      return analyzer.tokenStream(name(), new StringReader(stringValue()));\n     }\n \n     throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027": {
      "type": "Ymultichange(Ymovefromfile,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2011-09-22, 11:16 p.m.",
      "commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
      "commitAuthor": "Christopher John Male",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2011-09-22, 11:16 p.m.",
          "commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
          "commitAuthor": "Christopher John Male",
          "commitDateOld": "2011-09-22, 5:56 p.m.",
          "commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
          "commitAuthorOld": "Uwe Schindler",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }",
          "path": "lucene/src/java/org/apache/lucene/document/Field.java",
          "functionStartLine": 317,
          "functionName": "tokenStream",
          "diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS \u003d\u003d null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS \u003d new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData !\u003d null) {\n-        assert dataType !\u003d null;\n-        final Number val \u003d (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() \u003d\u003d null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used \u003d true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used \u003d false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream !\u003d null) {\n+      return tokenStream;\n+    } else if (readerValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldPath": "lucene/src/java/org/apache/lucene/document/NumericField.java",
            "newPath": "lucene/src/java/org/apache/lucene/document/Field.java",
            "oldMethodName": "tokenStreamValue",
            "newMethodName": "tokenStream"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2011-09-22, 11:16 p.m.",
          "commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
          "commitAuthor": "Christopher John Male",
          "commitDateOld": "2011-09-22, 5:56 p.m.",
          "commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
          "commitAuthorOld": "Uwe Schindler",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }",
          "path": "lucene/src/java/org/apache/lucene/document/Field.java",
          "functionStartLine": 317,
          "functionName": "tokenStream",
          "diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS \u003d\u003d null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS \u003d new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData !\u003d null) {\n-        assert dataType !\u003d null;\n-        final Number val \u003d (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() \u003d\u003d null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used \u003d true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used \u003d false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream !\u003d null) {\n+      return tokenStream;\n+    } else if (readerValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2011-09-22, 11:16 p.m.",
          "commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
          "commitAuthor": "Christopher John Male",
          "commitDateOld": "2011-09-22, 5:56 p.m.",
          "commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
          "commitAuthorOld": "Uwe Schindler",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }",
          "path": "lucene/src/java/org/apache/lucene/document/Field.java",
          "functionStartLine": 317,
          "functionName": "tokenStream",
          "diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS \u003d\u003d null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS \u003d new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData !\u003d null) {\n-        assert dataType !\u003d null;\n-        final Number val \u003d (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() \u003d\u003d null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used \u003d true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used \u003d false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream !\u003d null) {\n+      return tokenStream;\n+    } else if (readerValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2011-09-22, 11:16 p.m.",
          "commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
          "commitAuthor": "Christopher John Male",
          "commitDateOld": "2011-09-22, 5:56 p.m.",
          "commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
          "commitAuthorOld": "Uwe Schindler",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }",
          "path": "lucene/src/java/org/apache/lucene/document/Field.java",
          "functionStartLine": 317,
          "functionName": "tokenStream",
          "diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS \u003d\u003d null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS \u003d new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData !\u003d null) {\n-        assert dataType !\u003d null;\n-        final Number val \u003d (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() \u003d\u003d null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used \u003d true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used \u003d false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream !\u003d null) {\n+      return tokenStream;\n+    } else if (readerValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "tokenStreamValue",
            "newValue": "tokenStream"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "LUCENE-2309: Moved to Field.tokenStream(Analyzer)\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1174506 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "2011-09-22, 11:16 p.m.",
          "commitName": "5d4502ad0a3249fec5fcc1e28ce7074f67e8a027",
          "commitAuthor": "Christopher John Male",
          "commitDateOld": "2011-09-22, 5:56 p.m.",
          "commitNameOld": "c8b7bb7aac7f362f937bc9db86105b1d33c15048",
          "commitAuthorOld": "Uwe Schindler",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "actualSource": "  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n    if (!fieldType().indexed()) {\n      return null;\n    }\n\n    if (!fieldType().tokenized()) {\n      if (stringValue() \u003d\u003d null) {\n        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n      }\n\n      return new TokenStream() {\n        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n        boolean used;\n\n        @Override\n        public boolean incrementToken() throws IOException {\n          if (used) {\n            return false;\n          }\n          termAttribute.setEmpty().append(stringValue());\n          offsetAttribute.setOffset(0, stringValue().length());\n          used \u003d true;\n          return true;\n        }\n\n        @Override\n        public void reset() throws IOException {\n          used \u003d false;\n        }\n      };\n    }\n\n    if (tokenStream !\u003d null) {\n      return tokenStream;\n    } else if (readerValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), readerValue());\n    } else if (stringValue() !\u003d null) {\n      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n    }\n\n    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n  }",
          "path": "lucene/src/java/org/apache/lucene/document/Field.java",
          "functionStartLine": 317,
          "functionName": "tokenStream",
          "diff": "@@ -1,31 +1,43 @@\n-  public TokenStream tokenStreamValue() {\n-    if (!type.indexed()) return null;\n-    if (numericTS \u003d\u003d null) {\n-      // lazy init the TokenStream as it is heavy to instantiate\n-      // (attributes,...),\n-      // if not needed (stored field loading)\n-      numericTS \u003d new NumericTokenStream(precisionStep);\n-      // initialize value in TokenStream\n-      if (fieldsData !\u003d null) {\n-        assert dataType !\u003d null;\n-        final Number val \u003d (Number) fieldsData;\n-        switch (dataType) {\n-          case INT:\n-            numericTS.setIntValue(val.intValue());\n-            break;\n-          case LONG:\n-            numericTS.setLongValue(val.longValue());\n-            break;\n-          case FLOAT:\n-            numericTS.setFloatValue(val.floatValue());\n-            break;\n-          case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue());\n-            break;\n-          default:\n-            assert false : \"Should never get here\";\n-        }\n-      }\n+  public TokenStream tokenStream(Analyzer analyzer) throws IOException {\n+    if (!fieldType().indexed()) {\n+      return null;\n     }\n-    return numericTS;\n+\n+    if (!fieldType().tokenized()) {\n+      if (stringValue() \u003d\u003d null) {\n+        throw new IllegalArgumentException(\"Non-Tokenized Fields must have a String value\");\n+      }\n+\n+      return new TokenStream() {\n+        CharTermAttribute termAttribute \u003d addAttribute(CharTermAttribute.class);\n+        OffsetAttribute offsetAttribute \u003d addAttribute(OffsetAttribute.class);\n+        boolean used;\n+\n+        @Override\n+        public boolean incrementToken() throws IOException {\n+          if (used) {\n+            return false;\n+          }\n+          termAttribute.setEmpty().append(stringValue());\n+          offsetAttribute.setOffset(0, stringValue().length());\n+          used \u003d true;\n+          return true;\n+        }\n+\n+        @Override\n+        public void reset() throws IOException {\n+          used \u003d false;\n+        }\n+      };\n+    }\n+\n+    if (tokenStream !\u003d null) {\n+      return tokenStream;\n+    } else if (readerValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), readerValue());\n+    } else if (stringValue() !\u003d null) {\n+      return analyzer.reusableTokenStream(name(), new StringReader(stringValue()));\n+    }\n+\n+    throw new IllegalArgumentException(\"Field must have either TokenStream, String or Reader value\");\n   }\n\\ No newline at end of file\n",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[analyzer-Analyzer]"
          }
        }
      ]
    },
    "ffb3cbee57e1b075dac827bcc46bc95483c603e0": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2308: Moved over to using IndexableFieldType interface\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1167668 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2011-09-11, 12:07 a.m.",
      "commitName": "ffb3cbee57e1b075dac827bcc46bc95483c603e0",
      "commitAuthor": "Christopher John Male",
      "commitDateOld": "2011-08-27, 9:27 a.m.",
      "commitNameOld": "4dad0ba89f1d663939999be9005433dd629955f1",
      "commitAuthorOld": "Michael McCandless",
      "daysBetweenCommits": 14.61,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStreamValue() {\n    if (!type.indexed()) return null;\n    if (numericTS \u003d\u003d null) {\n      // lazy init the TokenStream as it is heavy to instantiate\n      // (attributes,...),\n      // if not needed (stored field loading)\n      numericTS \u003d new NumericTokenStream(precisionStep);\n      // initialize value in TokenStream\n      if (fieldsData !\u003d null) {\n        assert dataType !\u003d null;\n        final Number val \u003d (Number) fieldsData;\n        switch (dataType) {\n          case INT:\n            numericTS.setIntValue(val.intValue());\n            break;\n          case LONG:\n            numericTS.setLongValue(val.longValue());\n            break;\n          case FLOAT:\n            numericTS.setFloatValue(val.floatValue());\n            break;\n          case DOUBLE:\n            numericTS.setDoubleValue(val.doubleValue());\n            break;\n          default:\n            assert false : \"Should never get here\";\n        }\n      }\n    }\n    return numericTS;\n  }",
      "path": "lucene/src/java/org/apache/lucene/document/NumericField.java",
      "functionStartLine": 240,
      "functionName": "tokenStreamValue",
      "diff": "@@ -1,31 +1,31 @@\n   public TokenStream tokenStreamValue() {\n-    if (!indexed()) return null;\n+    if (!type.indexed()) return null;\n     if (numericTS \u003d\u003d null) {\n       // lazy init the TokenStream as it is heavy to instantiate\n       // (attributes,...),\n       // if not needed (stored field loading)\n       numericTS \u003d new NumericTokenStream(precisionStep);\n       // initialize value in TokenStream\n       if (fieldsData !\u003d null) {\n         assert dataType !\u003d null;\n         final Number val \u003d (Number) fieldsData;\n         switch (dataType) {\n           case INT:\n             numericTS.setIntValue(val.intValue());\n             break;\n           case LONG:\n             numericTS.setLongValue(val.longValue());\n             break;\n           case FLOAT:\n             numericTS.setFloatValue(val.floatValue());\n             break;\n           case DOUBLE:\n             numericTS.setDoubleValue(val.doubleValue());\n             break;\n           default:\n             assert false : \"Should never get here\";\n         }\n       }\n     }\n     return numericTS;\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "4dad0ba89f1d663939999be9005433dd629955f1": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2308: cutover to FieldType\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1162347 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2011-08-27, 9:27 a.m.",
      "commitName": "4dad0ba89f1d663939999be9005433dd629955f1",
      "commitAuthor": "Michael McCandless",
      "commitDateOld": "2011-07-12, 9:31 a.m.",
      "commitNameOld": "1c646d24c9f9118e770fca947c2bc5b68775f425",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 46.0,
      "commitsBetweenForRepo": 260,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStreamValue() {\n    if (!indexed()) return null;\n    if (numericTS \u003d\u003d null) {\n      // lazy init the TokenStream as it is heavy to instantiate\n      // (attributes,...),\n      // if not needed (stored field loading)\n      numericTS \u003d new NumericTokenStream(precisionStep);\n      // initialize value in TokenStream\n      if (fieldsData !\u003d null) {\n        assert dataType !\u003d null;\n        final Number val \u003d (Number) fieldsData;\n        switch (dataType) {\n          case INT:\n            numericTS.setIntValue(val.intValue());\n            break;\n          case LONG:\n            numericTS.setLongValue(val.longValue());\n            break;\n          case FLOAT:\n            numericTS.setFloatValue(val.floatValue());\n            break;\n          case DOUBLE:\n            numericTS.setDoubleValue(val.doubleValue());\n            break;\n          default:\n            assert false : \"Should never get here\";\n        }\n      }\n    }\n    return numericTS;\n  }",
      "path": "lucene/src/java/org/apache/lucene/document/NumericField.java",
      "functionStartLine": 240,
      "functionName": "tokenStreamValue",
      "diff": "@@ -1,27 +1,31 @@\n-  public TokenStream tokenStreamValue()   {\n-    if (!isIndexed())\n-      return null;\n+  public TokenStream tokenStreamValue() {\n+    if (!indexed()) return null;\n     if (numericTS \u003d\u003d null) {\n-      // lazy init the TokenStream as it is heavy to instantiate (attributes,...),\n+      // lazy init the TokenStream as it is heavy to instantiate\n+      // (attributes,...),\n       // if not needed (stored field loading)\n       numericTS \u003d new NumericTokenStream(precisionStep);\n       // initialize value in TokenStream\n       if (fieldsData !\u003d null) {\n-        assert type !\u003d null;\n+        assert dataType !\u003d null;\n         final Number val \u003d (Number) fieldsData;\n-        switch (type) {\n+        switch (dataType) {\n           case INT:\n-            numericTS.setIntValue(val.intValue()); break;\n+            numericTS.setIntValue(val.intValue());\n+            break;\n           case LONG:\n-            numericTS.setLongValue(val.longValue()); break;\n+            numericTS.setLongValue(val.longValue());\n+            break;\n           case FLOAT:\n-            numericTS.setFloatValue(val.floatValue()); break;\n+            numericTS.setFloatValue(val.floatValue());\n+            break;\n           case DOUBLE:\n-            numericTS.setDoubleValue(val.doubleValue()); break;\n+            numericTS.setDoubleValue(val.doubleValue());\n+            break;\n           default:\n             assert false : \"Should never get here\";\n         }\n       }\n     }\n     return numericTS;\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "400639f54e54ba2cf90b2436652450ded25861f7": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-3065, SOLR-2497: When a NumericField is retrieved from a Document loaded from IndexReader (or IndexSearcher), it will now come back as NumericField. Solr now uses NumericField solely (no more magic).\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1100526 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2011-05-07, 9:14 a.m.",
      "commitName": "400639f54e54ba2cf90b2436652450ded25861f7",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2011-02-10, 6:50 a.m.",
      "commitNameOld": "f4e977bb26143619a034574ef4a61e1dd136a3f4",
      "commitAuthorOld": "Uwe Schindler",
      "daysBetweenCommits": 86.06,
      "commitsBetweenForRepo": 535,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStreamValue()   {\n    if (!isIndexed())\n      return null;\n    if (numericTS \u003d\u003d null) {\n      // lazy init the TokenStream as it is heavy to instantiate (attributes,...),\n      // if not needed (stored field loading)\n      numericTS \u003d new NumericTokenStream(precisionStep);\n      // initialize value in TokenStream\n      if (fieldsData !\u003d null) {\n        assert type !\u003d null;\n        final Number val \u003d (Number) fieldsData;\n        switch (type) {\n          case INT:\n            numericTS.setIntValue(val.intValue()); break;\n          case LONG:\n            numericTS.setLongValue(val.longValue()); break;\n          case FLOAT:\n            numericTS.setFloatValue(val.floatValue()); break;\n          case DOUBLE:\n            numericTS.setDoubleValue(val.doubleValue()); break;\n          default:\n            assert false : \"Should never get here\";\n        }\n      }\n    }\n    return numericTS;\n  }",
      "path": "lucene/src/java/org/apache/lucene/document/NumericField.java",
      "functionStartLine": 200,
      "functionName": "tokenStreamValue",
      "diff": "@@ -1,3 +1,27 @@\n   public TokenStream tokenStreamValue()   {\n-    return isIndexed() ? numericTS : null;\n+    if (!isIndexed())\n+      return null;\n+    if (numericTS \u003d\u003d null) {\n+      // lazy init the TokenStream as it is heavy to instantiate (attributes,...),\n+      // if not needed (stored field loading)\n+      numericTS \u003d new NumericTokenStream(precisionStep);\n+      // initialize value in TokenStream\n+      if (fieldsData !\u003d null) {\n+        assert type !\u003d null;\n+        final Number val \u003d (Number) fieldsData;\n+        switch (type) {\n+          case INT:\n+            numericTS.setIntValue(val.intValue()); break;\n+          case LONG:\n+            numericTS.setLongValue(val.longValue()); break;\n+          case FLOAT:\n+            numericTS.setFloatValue(val.floatValue()); break;\n+          case DOUBLE:\n+            numericTS.setDoubleValue(val.doubleValue()); break;\n+          default:\n+            assert false : \"Should never get here\";\n+        }\n+      }\n+    }\n+    return numericTS;\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "778d96752fa94636a2136ea2b4d58a3fcbe283ec": {
      "type": "Yfilerename",
      "commitMessage": "SVN-GIT conversion, path copy emulation.\n",
      "commitDate": "2016-01-22, 7:18 p.m.",
      "commitName": "778d96752fa94636a2136ea2b4d58a3fcbe283ec",
      "commitAuthor": "Dawid Weiss",
      "commitDateOld": "2010-03-17, 10:57 a.m.",
      "commitNameOld": "2e5c6cdadc820220f8cb86e1b6e215da941649f9",
      "commitAuthorOld": "Uwe Schindler",
      "daysBetweenCommits": 2137.39,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStreamValue()   {\n    return isIndexed() ? numericTS : null;\n  }",
      "path": "lucene/src/java/org/apache/lucene/document/NumericField.java",
      "functionStartLine": 202,
      "functionName": "tokenStreamValue",
      "diff": "",
      "extendedDetails": {
        "oldPath": "src/java/org/apache/lucene/document/NumericField.java",
        "newPath": "lucene/src/java/org/apache/lucene/document/NumericField.java"
      }
    },
    "efb74380fda0da18650dbc66372a7bb1cd41dcf6": {
      "type": "Ybodychange",
      "commitMessage": "LUCENE-2285: Code cleanups to remove compiler warnings in eclipse.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@917019 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2010-02-27, 2:14 p.m.",
      "commitName": "efb74380fda0da18650dbc66372a7bb1cd41dcf6",
      "commitAuthor": "Uwe Schindler",
      "commitDateOld": "2010-01-31, 10:20 a.m.",
      "commitNameOld": "39b9f97cd414a6fbf7439091e1a42c83f1c71caf",
      "commitAuthorOld": "Robert Muir",
      "daysBetweenCommits": 27.16,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "actualSource": "  public TokenStream tokenStreamValue()   {\n    return isIndexed() ? numericTS : null;\n  }",
      "path": "src/java/org/apache/lucene/document/NumericField.java",
      "functionStartLine": 202,
      "functionName": "tokenStreamValue",
      "diff": "@@ -1,3 +1,3 @@\n   public TokenStream tokenStreamValue()   {\n-    return isIndexed() ? tokenStream : null;\n+    return isIndexed() ? numericTS : null;\n   }\n\\ No newline at end of file\n",
      "extendedDetails": {}
    },
    "edfce675a575bf83baecb52ca58c6e307d50eaad": {
      "type": "Yintroduced",
      "commitMessage": "LUCENE-1701, LUCENE-1687: Add NumericField , make plain text numeric parsers public in FieldCache, move trie parsers to FieldCache, merge ExtendedFieldCache and FieldCache\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/java/trunk@787723 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "2009-06-23, 11:42 a.m.",
      "commitName": "edfce675a575bf83baecb52ca58c6e307d50eaad",
      "commitAuthor": "Uwe Schindler",
      "diff": "@@ -0,0 +1,3 @@\n+  public TokenStream tokenStreamValue()   {\n+    return isIndexed() ? tokenStream : null;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public TokenStream tokenStreamValue()   {\n    return isIndexed() ? tokenStream : null;\n  }",
      "path": "src/java/org/apache/lucene/document/NumericField.java",
      "functionStartLine": 115,
      "functionName": "tokenStreamValue"
    }
  }
}